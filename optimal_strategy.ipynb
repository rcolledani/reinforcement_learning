{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP1_2021.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15rc1"
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NxgP4gIN5eF"
      },
      "source": [
        "**Recycling robot example** (from Sutton, page 42)\n",
        "References:\n",
        "  - Gym documentation: https://gym.openai.com/\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQ-0sEtFFcTM"
      },
      "source": [
        "import numpy as np\n",
        "from gym.envs.toy_text import discrete\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3dzvy9s6aX3"
      },
      "source": [
        "\n",
        "# TO DO: Explain shortly the gym environment"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPaZiYtu6aX6"
      },
      "source": [
        "# Consider the robot model described in Barto and Sutton Example 3.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U96qJdswGBFr"
      },
      "source": [
        "states = [\"high\", \"low\"]\n",
        "actions = [\"wait\", \"search\", \"recharge\"]\n",
        "\n",
        "P = {}\n",
        "\n",
        "P[0] = {}\n",
        "P[1] = {}\n",
        "\n",
        "alpha = 1\n",
        "beta = 1\n",
        "r_wait = 0.5\n",
        "r_search = 2.0\n",
        "\n",
        "# We define a discrete environment with the corresponding transitions\n",
        "def gen_ambient(alpha=alpha, beta=beta, r_wait=r_wait, r_search=r_search):\n",
        "    P[0][0] = [(1.0, 0, r_wait, False)]\n",
        "    P[0][1] = [(alpha, 0, r_search, False),\n",
        "               (1-alpha, 1, r_search, False)]\n",
        "    P[0][2] = [(1,0,0,False)]\n",
        "\n",
        "    P[1][0] = [(1.0, 1, r_wait, False)]\n",
        "    P[1][1] = [(beta, 1, r_search, False), \n",
        "               (1-beta, 0, -3.0, False)]\n",
        "    P[1][2] = [(1.0, 0, 0.0, False)]\n",
        "    env = discrete.DiscreteEnv(2, 3, P, [0.0, 1.0])\n",
        "    return(env)\n",
        "\n",
        "env = gen_ambient()"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eYJZZlHS-yJ",
        "outputId": "cef74cb1-7939-42b2-f565-6625cc8c56ae"
      },
      "source": [
        "print(P[0][1])\n",
        "print(env.P)"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(1, 0, 2.0, False), (0, 1, 2.0, False)]\n",
            "{0: {0: [(1.0, 0, 0.5, False)], 1: [(1, 0, 2.0, False), (0, 1, 2.0, False)], 2: [(1, 0, 0, False)]}, 1: {0: [(1.0, 1, 0.5, False)], 1: [(1, 1, 2.0, False), (0, 0, -3.0, False)], 2: [(1.0, 0, 0.0, False)]}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvcF7--Z6aX8"
      },
      "source": [
        "# Implement the random strategy for 20 steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoNiKgvOIC3n"
      },
      "source": [
        "Define a random action and see what reward it produces\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmwRqIM9lO-W"
      },
      "source": [
        "def random_strategy(env):\n",
        "  tps1 = time.clock()\n",
        "  \n",
        "  totalReward = 0\n",
        "  Reward = []\n",
        "  CumuReward =[]\n",
        "  observation = env.reset()\n",
        "  #print('The initial obsevation is :',  observation, 'so we are in the state', states[observation])\n",
        "  for i in range(20):\n",
        "    action = env.action_space.sample() # your agent here (this takes random actions)\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    totalReward += reward\n",
        "    Reward.append(reward)\n",
        "    CumuReward.append(totalReward)\n",
        "    #print('obsevation after step number ', i, 'is :',  observation, 'so we are in the state', states[observation],'The action taken is : ',  actions[action], ' and the reward  is: ', reward)\n",
        "  tps2 = time.clock()\n",
        "  timeConvergence = tps2 - tps1\n",
        "  return (totalReward,timeConvergence, Reward, CumuReward)\n",
        "\n",
        "totalReward , timeConvergence,Reward, CumuReward = random_strategy(env)"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9kpyAEk6aYB"
      },
      "source": [
        "# Plot the global reward "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "Xllib7oIo-MG",
        "outputId": "1260b93f-713a-46e6-b2ea-7874e8852b17"
      },
      "source": [
        "X = np.arange(20)\n",
        "plt.figure(1)\n",
        "plt.plot(X,Reward)\n",
        "plt.plot(X,CumuReward)"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff2ca9472d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBodHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAAIABJREFUeJzt3Xd8VFX6+PHPkxB6J3QIvSM10kTFBogoVkBpAgq42Nb2VXdX/enu6u6qu7q4AiqCgDQBC0VARbDQQm+hSoeEGnpIMs/vjzvsRkwgmXZnkuf9euWVmXvP3PPkZua5d8499xxRVYwxxuQfUW4HYIwxJrQs8RtjTD5jid8YY/IZS/zGGJPPWOI3xph8xhK/McbkM5b4jTEmn7HEb4wx+YwlfmOMyWcKuB1AVmJjY7VmzZpuh2GMMRFj5cqVR1S1fE7KhmXir1mzJgkJCW6HYYwxEUNEdue07BWbekSkuogsFJFNIrJRRJ7wLi8rIgtEZJv3d5lsXj/AW2abiAzI+Z9hjDEmGHLSxp8OPK2qjYF2wHARaQw8D3yrqvWAb73Pf0VEygIvA22BNsDL2R0gjDHGhMYVE7+qHlTVVd7Hp4DNQFWgBzDOW2wccGcWL+8CLFDVY6p6HFgAdA1E4MYYY3yTq149IlITaAksAyqq6kHvqkNAxSxeUhXYm+n5Pu+yrLY9REQSRCTh8OHDuQnLGGNMLuQ48YtIcWA68KSqnsy8Tp1B/f0a2F9VR6tqvKrGly+fowvTxhhjfJCjxC8iMThJf6KqzvAuThKRyt71lYHkLF66H6ie6Xk17zJjjDEuyUmvHgE+Ajar6tuZVn0JXOylMwD4IouXzwM6i0gZ70Xdzt5lxhhjXJKTfvzXAP2A9SKyxrvsReANYKqIDAZ2Az0BRCQeGKaqD6nqMRF5DVjhfd2rqnosoH+BMcaEg6RNsOlz8Gc624LFoOOTgYspGxKOc+7Gx8er3cBljIkYKftg1PVw9gggvm+neAV4ZqtPLxWRlaoan5OyYXnnrjHGRIy08zClH6SnwvAVUL6+2xFdkSV+Y4zxx9zn4MAq6DUxIpI+2Oicxhjju5VjYdU4uPYZaNTd7WhyzBK/Mcb4Yl8CzHkW6twEN7zodjS5YonfGGNy63Sy065fojLc8yFERbsdUa5YG78xxuRGRjpMGwjnjsHgBVC0rNsR5ZolfmOMyY0FL8HuH+Gu0VC5mdvR+MSaeowxJqfWfwZL34O2w6B5L7ej8ZklfmOMyYlDG+CLRyGuA3T+s9vR+MUSvzHGXMm54zClDxQuBfeNhegYtyPyi7XxG2PM5Xg8MP1hSNkPA+dAiaymHokslviNMeZyFr0B2xfAbW9D9TZuRxMQ1tRjjDHZ2TIXFv0NWvSF+EFuRxMwlviNMSYrR7bDjCFQuQXc9iaIH6NuhhlL/MYYc6nU087F3OgY6DUeYoq4HVFAWRu/McZkpgpfDIcjW6HfTCgd53ZEAXfFxC8iY4DuQLKqNvUumwI08BYpDZxQ1RZZvHYXcArIANJzOkmAMca45ud3nZm0bnkVandyO5qgyMkZ/1hgBPDJxQWq+t9b1kTkLSDlMq+/QVWP+BqgMcaEzM7v4ZtXoHEP6PC429EEzRUTv6ouFpGaWa3zTsTeE7gxsGEZY0wupZ2Dg2v9eP1Z+GwwxNaHHu/lqYu5l/K3jf9aIElVt2WzXoH5IqLAKFUd7Wd9xhjzWxfOwpgucGidf9spVBJ6TYBCJQITV5jyN/HfD0y6zPqOqrpfRCoAC0QkUVUXZ1VQRIYAQwDi4vLexRRjTJCowqzfw6H1zk1WZWv5vq3YBlCqauBiC1M+J34RKQDcDbTOroyq7vf+ThaRmUAbIMvE7/02MBogPj5efY3LGJPPLP8A1k2GG/4AVw92O5qI4E8//puBRFXdl9VKESkmIiUuPgY6Axv8qM8YY35t988w7wVo0M2Z99bkyBUTv4hMApYADURkn4hcPKT25pJmHhGpIiJzvE8rAj+KyFpgOTBbVb8OXOjGmHzt5EGYOgBK14C7RkKU3Y+aUznp1XN/NssfzGLZAaCb9/FOoLmf8RljzG+lX4Cp/eHCGRjwpTNcsskxu3PXGBN55r0A+5Y7Y+NXaOR2NBHHvhsZYyLL6omw4kO45glocpfb0UQkS/zGmMhxYLXTdbPW9XDjS25HE7Es8RtjIsOZozClHxSvAPeOgWhrqfaV7TljTPjLSIfPBsLpZBg8D4rFuh1RRLPEb4wJf9+9Cr8sgh7/gSot3Y4m4llTjzEmvG2cCT+9A/GDoWUft6PJEyzxG2PCV3IifD4cql0NXd9wO5o8wxK/MSY8nU9xpj8sWAx6jocCBd2OKM+wNn5jTPjxeGDmMDi+CwZ8BSUrux1RnmKJ3xgTfn54C7bMgVv/DjU6uB1NnmNNPcaY8LJtASz8CzTrBW2GuB1NnmSJ3xgTPo7thOmDoVJT6P6vPD39oZss8RtjwsOFMzC5LyDO9IcFi7odUZ5lbfzGGN+pOlMeJs6CxNnOGbuvPBmQcQH6fgZlagYsRPNblviNMbnjyYC9y2DzLEj8Ck7sAQTi2kP8IP+aZ+LaQ92bAxaqydoVE7+IjAG6A8mq2tS77BXgYeCwt9iLqjoni9d2Bd4BooEPVdXuwDAmEqWnws5FTqLfMhfOHIboglC7kzPlYYNuULy821GaHMrJGf9YYATwySXL/6mqb2b3IhGJBt4DbgH2AStE5EtV3eRjrMaYUDp/ErYvcM7sty2AC6egYAmodws06g51b4HCJd2O0vggJ1MvLhaRmj5suw2w3TsFIyIyGegBWOI3JlydPgxbZjvt9Tu/d9rci8ZC07ug4e1Q+3ooUMjtKI2f/Gnjf1RE+gMJwNOqevyS9VWBvZme7wPa+lGfMSZYVGHuc7D8A0ChdBxc/bBzZl+9LURFux2hCSBfE//7wGuAen+/BQzyJxARGQIMAYiLi/NnU8aY3Fr+ASwfDS37QpuhUOkq60Ofh/nUj19Vk1Q1Q1U9wAc4zTqX2g9Uz/S8mndZdtscrarxqhpfvrxdJDImZHb/7ExeXr8r3P5vqNzMkn4e51PiF5HMIybdBWzIotgKoJ6I1BKRgkBv4Etf6jPGBMnJgzB1AJSuAXeNgii7pzM/yEl3zklAJyBWRPYBLwOdRKQFTlPPLmCot2wVnG6b3VQ1XUQeBebhdOcco6obg/JXGGNyL/0CTO3v3DHb/wsoUtrtiEyI5KRXz/1ZLP4om7IHgG6Zns8BftO/3xgTBua9APuWw70fQ8XGbkdjQsi+1xmTH62eCCs+hA6PQdO73Y7GhJglfmPymwOrYdbvodZ1cNMrbkdjXGCJ35j85MxRmNIPipV3mniibbiu/Mj+68bkFxnp8NlAOJ0Mg76GYrFuR2RcYonfmPziu1fhl0XQ4z2o2srtaIyLrKnHmPxg40z46R1n2OSWfd2OxrjMEr8xeV1yInw+HKpdDV3/5nY0JgxY4jcmLzufApMfgILFoOcnUKCg2xGZMGBt/MbkVR4PzBwGJ3bDgK+gZBW3IzJhwhK/MXnVD2/Bljlw69+hRge3ozFhxJp6jMmLti2AhX+BZr2gzRC3ozFhxhK/MXnNsZ0wfTBUagrd/2VDLJvfsMRvTF5y4QxM7gsI9JoABYu6HZEJQ9bGb0xeoQpfPQHJm6DvZ1CmptsRmTBlZ/zG5BVL34f10+DGP0Ldm92OxoQxS/zG5AW7foT5f4SG3eHap92OxoQ5S/zGRLqU/TDtQShbG+583y7mmiu6YuIXkTEikiwiGzIt+4eIJIrIOhGZKSJZztkmIrtEZL2IrBGRhEAGbowB0lOd6RPTzkHviVC4pNsRmQiQkzP+sUDXS5YtAJqqajNgK/DCZV5/g6q2UNV430I0xmRr7nOwP8E50y/fwO1oTIS4YuJX1cXAsUuWzVfVdO/TpUC1IMRmjLmcleNg5Vjo+HtofIfb0ZgIEog2/kHA3GzWKTBfRFaKyGVvHxSRISKSICIJhw8fDkBYxuRh+1bCnGeg9g1w45/cjsZEGL8Sv4j8AUgHJmZTpKOqtgJuBYaLyHXZbUtVR6tqvKrGly9f3p+wjMnbTh+Gqf2geCW4dwxERbsdkYkwPid+EXkQ6A70UVXNqoyq7vf+TgZmAm18rc8Yw/+mTzx7FHqNh6Jl3Y7IRCCfEr+IdAWeA+5Q1bPZlCkmIiUuPgY6AxuyKmuMyaFvXoZdP0D3f0KVFm5HYyJUTrpzTgKWAA1EZJ+IDAZGACWABd6umiO9ZauIyBzvSysCP4rIWmA5MFtVvw7KX2FMfrBhOiwZAVc/DC0ecDsaE8GuOFaPqt6fxeKPsil7AOjmfbwTaO5XdMYYR9Im+OJRqN4OuvzV7WhMhLM7d40Jd+dOwJQ+UKgE9Bxn0ycav9nonMaEM48HZgyBE3vgwdlQopLbEZk8wBK/MeFs8d9h2zzo9ibEtXM7GpNHWFOPMeFqy9fw/evQ/H64+iG3ozF5iCV+Y8LR0R1OE0+lZk7XTRtx0wSQJX5jwk3qaZjSF6KinOkTY4q4HZHJY6yN35hwogpfPgaHE6HvdChTw+2ITB5kZ/zGhJMl78HGGc7Aa3VudDsak0fZGb8x4cDjgS2zYcFL0Oh2Z6hlY4LEEr8xbkm/4Iy7kzgLEufA6UMQ28CmTzRBZ4nfmFC6cAa2fwObZ8HWeZCaAjFFoe7Nzpl+g1udO3SNCSJL/MYE29ljsGWuc2a/4ztIPw9FykCj7tCwO9S5wXrumJCyxG9MMKTsg8TZsPkr2P0zaAaUrAatBjgJP64DRNvHz7jD3nnGBMrhLU6iT5wFB1Y7y2IbQMcnnTP7Ki2t7d6EBUv8xvjK43ESfOJXTpv90W3O8qrxcPMrTrKPredmhMZkyRK/MbmRkQa7f3ISfeJsOHUAogpAzY7Qdig0vA1KVnE7SmMuK0eJX0TG4Myvm6yqTb3LygJTgJrALqCnqh7P4rUDgD96n/5ZVcf5H7YxIXThrHNRNnGWc5H2/AkoUATq3gSNXob6XZyLtcZEiJye8Y/FmW7xk0zLnge+VdU3ROR57/P/y/wi78HhZSAeUGCliHyZ1QHCmLBy7rjT3XLzV7D9W0g/B4VLQf1bnYuzdW6CgkXdjtIYn+Qo8avqYhGpecniHkAn7+NxwPdckviBLsACVT0GICILgK7AJJ+iNeZyzp+Eo9v92IDC/lXOmf2uH8GTDiUqQ8s+Tnt9zY4QHROwcI1xiz9t/BVV9aD38SGcydUvVRXYm+n5Pu8yYwLr9GEY3QlO7vN/W+XqQvtHnRuqqrRyRsk0Jg8JyMVdVVURUX+2ISJDgCEAcXFxgQjL5BcZaTDtQTh7FO4a7TTJ+KpsLYitb90uTZ7mT+JPEpHKqnpQRCoDyVmU2c//moMAquE0Cf2Gqo4GRgPEx8f7dRAx+cyCl2H3j07Sb97L7WiMCXv+fIf9EhjgfTwA+CKLMvOAziJSRkTKAJ29y4wJjPWfwdL3oO0wS/rG5FCOEr+ITAKWAA1EZJ+IDAbeAG4RkW3Azd7niEi8iHwI4L2o+xqwwvvz6sULvcb47dAG+OJRZ/iDzn92OxpjIoaohl+rSnx8vCYkJLgdhgln5447F3PTU2HIIiiRVd8CY/IPEVmpqvE5KWt37prI4/HA9IchZT8MnGNJ35hcssRvIs+iN2D7Arjtbajexu1ojIk41kHZRJbEObDob9CiL8QPcjsaYyKSJX4TOY5sh5lDneGNb3vL+tob4yNL/CYypJ6GKX2cIRN6joeYwm5HZEzEsjZ+E/5U4YvfwZGt0G8mlK7udkTGRDRL/Cb8/fwubPoCbnkVandyOxpjIp419ZjwtvN7+OYVaHwndHjc7WiMyRMs8ZvwdWIPTBvoDJrW4z27mGtMgFjiN+Ep7RxM6eeMid9rIhQq7nZExuQZ1sZvwo8qzH4aDq6B3pMgtq7bERmTp9gZvwk/CWNgzUS4/v+gYTe3ozEmz7HEb8LL3uUw9/+gXme4/nm3ozEmT7LEb8LHqSSnXb9UNbh7tE15aEyQWBu/cZcqHFjtTHC+fhqknoS+06FIGbcjMybPssRvQi8jHfb8DJtnQeJsZ4J0iYYaHeD2d6BSU7cjNCZP8znxi0gDYEqmRbWBl1T1X5nKdMKZkvEX76IZqvqqr3WaCJZ2DnYsdM7st8yFc8egQGGocyPc8CI0uBWKlnU7SmPyBZ8Tv6puAVoAiEg0zsTqM7Mo+oOqdve1HhPBzp2AbfNh81ew/VtIOwOFS0H9rtCwO9S9CQoWcztKY/KdQDX13ATsUNXdAdqeiVSnDjnNN4mz4JfFzg1YxStB897QqDvUvNYZYdMY45pAJf7ewKRs1rUXkbXAAeAZVd0YoDpNoBzZDolfOU0wp5N834564MReQKFsHWg/HBreDlVbWw8dY8KI34lfRAoCdwAvZLF6FVBDVU+LSDfgc6BeNtsZAgwBiIuL8zcsczmqzl2xm2c5Z+aHE53llVtA9bb+bbtlf+fMvnxDG1vHmDAViDP+W4FVqvqbU0VVPZnp8RwR+Y+IxKrqkSzKjgZGA8THx2sA4jKZZaTDniVOok+cDSl7QaKgxjXQeiA0vM3GuTcmnwhE4r+fbJp5RKQSkKSqKiJtcG4YOxqAOk1OpJ2HnQudM/utc+HsUYgu5PSk6fQ81L8VipVzO0pjTIj5lfhFpBhwCzA007JhAKo6ErgXeERE0oFzQG9VtbP5YDqfAlvnO232275xetIUKgn1u3h70txsI10ak8/5lfhV9QxQ7pJlIzM9HgGM8KcOk0OeDPjqcVg7BTxpULwiNOvp7UlzHRQo6HaExpgwYXfu5hUL/wqrJ0D8IGjWG6pdbT1pjDFZssSfF2yeBT+8Ca36Q/d/uh2NMSbM2SlhpDu8FWYOgyqt4NZ/uB2NMSYCWOKPZKmnYEofKFAIeo2HmMJuR2SMiQDW1BOpVOHzR+DoDuj/uTOGvTHG5IAl/kj14z+dwc86/wVqXed2NMaYCGJNPZFo+7fw3WvQ9B5nPBxjjMkFS/yR5vhumD7YGQvnjn/beDjGmFyzxB9J0s7BlL7OKJi9JthY9sYYn1gbf6RQhVm/h0Pr4YGpUK6O2xEZYyKUnfFHihUfwtpJ0OkFqN/Z7WiMMRHMEn8k2LMUvn7embLwumfdjsYYE+Es8Ye7U4dgan8oXQPuGmXj7xhj/GZt/OEs/YKT9FNPQ7/PoUhptyMyxuQBlvjD2bwXYe8yuPdjqNjY7WiMMXmEtRuEqzWfwooPoMNj0PRut6MxxuQhlvjD0YE1TtfNWtfBTa+4HY0xJo/xO/GLyC4RWS8ia0QkIYv1IiLvish2EVknIq38rTNPO3MUpvSDorFOE0+0tcYZYwIrUFnlBlU9ks26W4F63p+2wPve3+ZSngyYPghOJ8Ggr6FYrNsRGWPyoFA09fQAPlHHUqC0iFQOQb2R5dgvTvPOzu+h+9tQ1b4YGWOCIxBn/ArMFxEFRqnq6EvWVwX2Znq+z7vsYOZCIjIEGAIQFxcXgLDCnCokbXCmTUyc5TwGaPsItOzrbmzGmDwtEIm/o6ruF5EKwAIRSVTVxbndiPeAMRogPj5eAxBX+PFkwN7lTqJPnAXHdwECce2ccfUbdYcyNV0O0hiT1/md+FV1v/d3sojMBNoAmRP/fqB6pufVvMvyh/RU+GWxM2nKlrlwJhmiC0Kt66Hj76FBNyhewe0ojTH5iF+JX0SKAVGqesr7uDPw6iXFvgQeFZHJOBd1U1T1IHlZ6mnYvsBpxtk2H1JPQsHiUO8WaNgd6nWGwiXdjtIYk0/5e8ZfEZgpzmQgBYBPVfVrERkGoKojgTlAN2A7cBYY6Ged4StxDqwaBzsWQkaq0yWzcQ9odLtzhm+ToRtjwoBfiV9VdwLNs1g+MtNjBfL+/IBbvobJ90Op6nD1YOfMPq4dREW7HZkxxvyK3R0UCEd3wIwhUKkZDJ4PMUXcjsgYY7JlQzb4K/W0Mx1iVJQzHaIlfWNMmLMzfn+owpePweFE6DsdytRwOyJjjLkiO+P3x5L3YOMMuPFPUOdGt6MxxpgcscTvq18Ww4KXoNEdTn98Y4yJEJb4fZGyD6YNhHJ14M7/gNOd1RhjIoIl/txKO+8Mm5yeCr0mQqESbkdkjDG5Yhd3c2vus3BglZP0y9d3OxpjjMk1O+PPjYSPYdUncO0zzoBqxhgTgSzx59TeFTDnWahzE9zwotvRGGOMzyzx58TpZJjaH0pWgXs+tGEYjDERzdr4ryQjzenBc+64MxxD0bJuR2SMMX6xxH8lC16C3T/CXaOhcjO3ozHGGL9ZU8/lrJsGS/8DbYdB815uR2OMMQFhiT87hzY44/DEdYDOf3Y7GmOMCRhL/Fk5ewym9IEipeG+sRAd43ZExhgTMD4nfhGpLiILRWSTiGwUkSeyKNNJRFJEZI335yX/wg0BTwbMeBhS9kPPT6BERbcjMsaYgPLn4m468LSqrhKREsBKEVmgqpsuKfeDqkbO3U7fvwHbv4Hb3obqbdyOxhhjAs7nM35VPaiqq7yPTwGbgaqBCswViXNg8d+hRV+IH+R2NMYYExQBaeMXkZpAS2BZFqvbi8haEZkrIk0CUV9QHNkGM4dC5RZw21s24qYxJs/yux+/iBQHpgNPqurJS1avAmqo6mkR6QZ8DtTLZjtDgCEAcXFx/oaVO6mnnOkTo2Og13iIKRza+o0xJoT8OuMXkRicpD9RVWdcul5VT6rqae/jOUCMiMRmtS1VHa2q8aoaX758eX/Cyh1V+GI4HNkK946B0iE+6BhjTIj506tHgI+Azar6djZlKnnLISJtvPUd9bXOoPj5Xdj0Bdz8CtTu5G4sxhgTAv409VwD9APWi8ga77IXgTgAVR0J3As8IiLpwDmgt6qqH3UG1o6F8M0r0PhO6PA45y5kcOLcBZ83JwgVSxZCXLo+kJqeQUxUFFFR+fP6RFqGhygRovPp35+e4eHw6VS/thFbvBAx0ZF5e4/Ho6R7lIIF3IlfVUnLcK/+3PA58avqj8BlP2GqOgIY4WsdQXViD3w2CGLrQ4/3WLsvhQc/Xs7xs2l+bfbaerGM6teaogVDOwzSlkOn6PfRMmrFFuPDAfGUKJy/bjrbc/Qs/cYso2LJwkwY3DYiPnyBlHzyPP3HLCfx0Cm/tlOzXFHGD25L9bJFAxRZaKRneBg8LoE9x84y45EOlClWMKT1ezzKsAkr2XjgJDOHd6BCifC+TijhdAJ+UXx8vCYkJASvgrRzMKYLHPsFHl7IspNlGDwugTLFYnjk+rr4esJ46OR53v12Gy3jyjDmwaspVSQ0yXfdvhP0H7OcaBFSzqXRuEpJxg1sE/I3v1u2J5+iz4fLOJOawenUdPq3r8GrPZq6HVbI7Dt+lr4fLiP5VCpP3VKf4oV8O+lITffw1vwtFCtUgAkPtaVO+eIBjjR4Xp+7mVGLdhIdJXSoU46xA9uE9JvfPxds5Z1vtxEdJbSuUYaJD7UN+TcnEVmpqvE5KZv/RudUhdlPw8G10HsSi46VYuj45VQtXYSJD7WjUin/jtT1K5bgicmreeCDpYwf3JayQU6+y385xqCxKyhdNIZPH2rHtuRTPDJxFb1HL2X8Q23C/szDXxv2p9B/zHKiRJj+SAemr9rH6MU7aVatNPe2ruZ2eEG38/Bp+n64jNOp6Ux4qC2t4sr4tb2ra5al30fL6DVqCeMHt6VR5ZIBijR4Zq87yKhFO+nXrgZNqpTk+RnreWv+Fp7r2jAk9X+7OYl3vt3Gva2r0bFuLE9OWcNf52zm5dvDt/d6/vo+DJDwEayZCNc9x9fpLXlo3ApqxxZn6tD2fid9gG5XVWZ0/3i2J5+m16glJJ08H4Cgs7Zo62H6j1lGhZKFmDasPXHlinJTo4qMffBq9h4/S8+RS9h3/GzQ6nfbyt3Huf+DpRQuEMW0Ye1pUKkEz3VpQIc65Xhx5no27E9xO8SgSjx0kp6jlpKa7mHSkHZ+J32AxlVKMnVYe2Kio+g1agmr9xwPQKTBszXpFM9+tpZWcaX5U/fG9G4Tx/1tqvOf73fw9YaDQa//lyNneHLKGppWLcmf72zKnS2rMvCamnz80y4+X70/6PX7Kn8l/j3LYO7zUK8zM0r1Y/inq7mqaikmDWlHueKFAlbNDQ0qMHZgGw6cOMd9I5ew91jgk+/XGw7x0LgV1PIetCqXKvLfdR3qxjJ+cFuOnrlAz5FL+OXImYDX77afth+h30fLKFesINMe6UCt2GIAFIiO4t/3tyS2WEGGjl/JsTO+X6wPZ2v2nqDXqKUUiBKmDG1PkyqlArbtOuWd91TpogXp++EyluwIr454F508n8bQ8SspWrAA7/dt/d/rOq/c0YTm1Uvz9NS1bE8+HbT6z6SmM2z8SgpECSP7tqZwjDMz34vdGtGmZlmen7GOTQcuvbUpPOSfxH8qyZk+sVRVpsa9xFPT1tO2VlnGD24blLb49nXKMeGhtqScS+O+kUsC+gacuXofwz9dRdOqpZj8cDtiszhota5RhslD2pGa7uG+kUtIPBSeb0BffLMpiYFjV1C9TFGmDmtP1dJFfrW+XPFCjOzXmsOnU3l80moyPOF3HcsfS3cepc8HSylVJIZpw9pTt0Lg2+Krly3KtGHtqVK6CA9+vJyFickBr8MfHo/y1JS17D12lv/0aUXFkv/7tl6oQDTv92lF4Zhoho5P4NR5/zpsZEVVeW76OrYln+Ld+1tSrcz/LobHREcxok9LShWJYeiEBE6cDb+Tj/yR+NMvwLQBkHqSaXXf4LnZe7i5UQXGPHg1xXy8EJYTLeOc5Jvu8dBr1JKAHP0nLN3NU1PX0rZWWSYMbkupotkftJpUKcWUoe2JjoLeo5eydu8Jv+t321drDzBswkoaVirB5CHtsr2G0axaaf7coyk/bj/CP+ZtCXGUwfP9lmQGjFlO5dJFmDq0fVB731QsWZgpQ9tTr2JxhoxPYM764Ded5NSIhdv5ZnMSf7ytEW1q/XY61CqlizDigVbsOnqWZ6atxRPgg/+HP/zC7HUHebZLQ66t99sbTiuUKMz7fVtzKOU8j09eE3YnH/kj8c//I+xZwuyaL/B5iRgaAAASTklEQVTsDx66N6vM+5m+mgVTo8olmTq0PQULRNF79BJW+dFmOnrxDv74+QZuaJDzg1bdCsWZNrQDJQoXoM+Hy1i2Mzy/tufElBV7eHzyalrFOb0mrtRrqefV1XmgbRwjF+1gbhglLV/NXX+Qhz9JoG6F4kwZ4n9HhJwoW6wgnz7cjubVSvPop6v4bOW+oNd5JQsTk/nnN1u5u2VVBnSomW259nXK8cKtDZm3MYn3F+0IWP0/bz/C63M3c2vTSgy7vna25VrFleGVO5qweOth/vXN1oDVHwh5P/GvnQzLR7G0Ym+Gr69Dz/hqvNO7ZUi7WtX2tpmWKea0mf6840iuXq+qvL1gK3+dk8htzSr/qj0xJ+LKFWXa0A5ULFmIAR8v5/st4fW1PSfG/PgL/zd9PdfWK8+4QW1yfJ/Cy7c3pmVcaZ6ZtpZtSf71cXfT9JVO816zaqX59OHAXpO6kpKFY/hkcBs61InlmWlr+WTJrpDVfandR8/wxOTVNKpUkr/cddUVb5Yc3LEWtzevwpvzt7B462G/699/4hyPTlpN7fLF+cd9za9Y/wNt4ugZX41/f7ed+RsP+V1/oOTtxH9wLfrVE+ws1oK+u7sx8JqavHF3M1fu7KxetijThranWpkiDPx4RY7bTFWVP8/ezLvfbqNnfDXe7d3Sp5uTKpUqzNSh7akdW5yHP0kISY+HQFBVRny3jVdnbaJLk4p80L81RQrm/KDntPc6rxk6fiUng9DeG2zjl+7m6WlraV+nHJ8MahOy+0MyK1qwAB8OiOfmRhV56YuN/Of77SGP4eyFdIaOX4mIMKpfzt4HIsLf7rmKBhVL8Pjk1X51tDiflsEjE1ZyId3DqH6tc3S/hIjwao+mNKtWiqemrmXH4eBdbM6NvJv4zx5Dp/QlheL0PDqUR25syEvdG7s6nEGFkoWZPOR/baaz110++WZ4lBdnruejH3/hwQ7+H7TKFS/EpCHtuKpqKYZ/upoZq9z/2n45qsrfvt7Cm/Odr/XvPdCKQgVy3zxXqVRh3nugFbuPneXpqYFv7w2mUYt28KfPN3Bzowp8NCC416SupHBMNO/3bcUdzavw96+38I95iYTqBlBV5fnp69mS5FxMzc21jaIFCzCqX2s8HmXo+JWcu5DhU/0vfbGBdftSeLtn81zd3ObsN6fX0dDxKzmdmp7r+gMtbyZ+TwYZnw0i/cRBHjzzOIO7tuXpzg1cG0Mns8xtpo9NWsW0hL1ZlkvL8PDU1DVMWr6X4TfU4eXbA3PQKlUkhvGD29K2VlmemrqW8Ut3+73NYPB4lJe+2MjIRTvo0zaON+9rTgE/mufa1i7HH7o1YsGmJFfOVnNLVXlr/hZen5vI7c2rhOya1JXEREfxz14t6H11dd5buIP/99WmkBxIx/y0iy/XHuCZzg24vn7uR++tUa4Y7/RuyeZDJ/nDzPW5PmBNWr6XqQn7eOzGunRuUinX9VctXYQR97dk5+HTPDttbcgOmNnJk4k/7ZvXiN65kD+lDeCeO3rwSKc6bof0KxfbTK+pG8uzn61j3M+7frX+fFoGv5u4ii/WHOC5rg14tkvDgB60ihUqwJgHr+amhhX40+cbGBXAC1+BkJ7h4ZnPnIPS0Otq8+c7mwbkoDfwmprc2aIKby3YGtbXOVSVV2dt4t/fbadXfHX+1atFWA2cFh0lvH73VQzuWIuxP+/i/6avC2qvlaU7j/LXOZvp0qQiv/Pjs3xDwwo8eVN9ZqzezydLcn7Cs2rPcV7+cgPX1y/PkzfX97n+DnVjeeHWRszdcIhRi3f6vJ1ACJ93U4CcWfs5MT//k8kZN3D13b+nX/uaboeUpYttpp0bV+TlLzfy3kLnLPTshXQeGpfAgk1JvNqjCb/rVDco9ReOiWZkv9Z0b1aZ1+cm8vb8La6fhQBcSPfw2KTVzFi1n6duqc/ztwbuoCcivH53MxpWKskTk9ew52j43dWc4XGaND7+aReDrqnFG/dcFZajjYoIf7ytEY/fVI9pK/fx+OTVXEj3BLyegynnePTTVdQoV5Q3c3Ax9Uoeu7EuNzeqwGuzNrFi17Erlj98KpVHJqykUqnCvNO7hd//i4eurcVtzSrz968T+WGb/xebfZWnEv+JPRuRmcNY66lDmXv+xT1hPlZLoQLRvNenFT1aVOEf87bw1zmb6f/Rcn7ecYQ372tO/yAftGKio3ind0vnovF323lt1mZXk/+5Cxk8/EkCczcc4k/dG/P4TfUC3jxXpGA0o/q2RlUZOsG39t5gScvw8OSUNUxJ2MvjN9blT90bhUXzZHZEhKduqc+L3Roye91Bhk1Yyfm0wO3P1PQMHpmwinMXMhjdr3VARpyNihLe7tWC6mWL8ruJqy47pEpahofhn64i5Vwao/rGU7qo/+NuiQh/v6cZ9SqU4PFJ/l1s9iuOcDjLu5Qvo3OmHD9Gyr+vpVjGSbb0mEWHVs2DFF3gZXiUP36+gUnL9xATLbzTuyXdrqocsvo9HqdpYezPu2hdowy3XVWZzk0q/upuxGBRVbYknWLehiS+XLufnUfO8PpdV9G7TXBnQvt+SzIDx67gjuZV+FevFn4l2O3Jp5m38RDzNyVx4MQ5n7dzId1Dyrk0nr+1IcOuD6/mySuZsHQ3f/piA3XKF+eO5lXo0qQS9SsW92u/vjBjPZOW72Fk39Z0bZr7dvXL2Zp0ijvf+8l7I2D7LHvK/b+vNvLxT7v4V68W3NmyakDr/+XIGe4Y8SM1yhXls2EdAnL9Jjejc+aZxO9JS2X56OGUbHknjTt0D1JkwaOqjF+6m7oVitOhTpazUwa9/rE/72LS8j1sTXK6nDWtWpIujSvRpWkl6lXw70OcmcejrN57nHkbk5i38RC7j55FxLnhZch1teniw8UzX4z4bhtvzt/Ky7c3ZuA1tXL8OlVl3b4U5m08xLyNh9hx2BkLqXm1UjSuUpIrTFNxWR3qlOP25lV8fr2bvt5wkNGLd7Jqj3OHeM1yRenSpBKdm1SiZfXSubpOM3n5Hp6fsZ7fdaoTtFE2Z687yPBPV9GvXQ1eu/PXw3h/vno/T05Zw8BragZtlM1vNycxeFwC97Sqxpv3NfP78xWyxC8iXYF3gGjgQ1V945L1hYBPgNY4Uy72UtVdV9pu0MfjN5f1y5Ez/01qq70f4lqxxejcpCJdmlSiRbXcfYjBOZtdsvMo8zYeYsGmJA6fSiUmWmhfJ5YuTSpyS+OKIR9C2uNxmnu+S0zm04fa0rZ2uWzLpmd4WP7Lsf+e2R9MOU90lNC2Vllvcqv4q4Hy8rPkk+eZv8k5qC/ZcZR0j1KhRCFuaey8f9rVLnfZe1HW7D1Bz5FLaFu7bNDH1X99zmZGLd7JP+5txn3x1QHYdOAkd7//E82qlmbiw8EdV//iOP6v9Wji9/XIkCR+EYkGtgK3APuAFcD9qropU5nfAc1UdZiI9AbuUtVeV9q2Jf7wkeT9EM/P9CGuWPLXH+LsPhhnUtNZtPUw8zYe4rvEZE6dT6dowWg6NShPlyaVuKFhBUq6PFPYyfNp3DniJ06eT2PWY9f+ahiE82kZLN56mHkbk/g2MYkTZ9MoVCCK6+o78d/UsEK+mezGVynn0liYmMy8jYf4fsthzqVlUKJwAW5uVJEuTSpyXf3yv5qt7sjpVG7/949ERwlfPdox6Ps3PcND/zHLSdh9nOnDOlC9bBFuH/EjF9I9fPVYx6CfjHg8ysOfJLBo62GmDG1H6xq/HXcop0KV+NsDr6hqF+/zFwBU9fVMZeZ5yywRkQLAIaD8lebdtcQfnlLOpvHdliTmbUhi0VbnQ1yycAFuyvQhPp/m4ZvNzoFi8bYjXEj3UKZojPeDXomO9WLDoj96Ztu87b31K5VgdL94ftx++LJ/Y6in1cwrzqdl8MO2I8zbeIhvNv/2QNqpQXke/XQVq/ecYPojHWhaNXBDTV/O0dOp3DHiJ8D5Zrvsl6NMGdo+IPMb5ETKuTTuGPEj5y5kMOuxjlQo6dvBJlSJ/16gq6o+5H3eD2irqo9mKrPBW2af9/kOb5nLDlZjiT/8nbuQwQ/bfns2nJbhwaNQpVRhOjepRJcmlbi6Zhm/br4KhbnrD/LIxFX/fV6hRKH/Nm1d7luN8U16hoflu44x33ud52DK/3rXvN2zOXe3Cm2PvHX7TnDvyCVcSPfwl7ua0qdtjZDWv+WQc/LRpEpJPn24nU/DskRk4heRIcAQgLi4uNa7d4fnHaXmty62f3+zOZmiBaPp0qQSTauWDOuuiFmZsHQ3+46fo3OTij5dxzC+UVXW73culpctVojBHXN+oT2QvktMYufhMwzuWMuV9+5Xaw/w844jvHJHE5+GJrGmHmOMyWdyk/j9+f66AqgnIrVEpCDQG/jykjJfAgO8j+8FvrtS0jfGGBNcPl+lUtV0EXkUmIfTnXOMqm4UkVeBBFX9EvgIGC8i24FjOAcHY4wxLvKre4KqzgHmXLLspUyPzwP3+VOHMcaYwLKuCsYYk89Y4jfGmHzGEr8xxuQzlviNMSafscRvjDH5TFgOyywihwFfb92NBS47JITLLD7/WHz+sfj8E87x1VDVHE1IHJaJ3x8ikpDTu9fcYPH5x+Lzj8Xnn3CPL6esqccYY/IZS/zGGJPP5MXEP9rtAK7A4vOPxecfi88/4R5fjuS5Nn5jjDGXlxfP+I0xxlxGxCZ+EekqIltEZLuIPJ/F+kIiMsW7fpmI1AxhbNVFZKGIbBKRjSLyRBZlOolIiois8f68lNW2ghjjLhFZ7637N5MfiONd7/5bJyKtQhhbg0z7ZY2InBSRJy8pE9L9JyJjRCTZO7nQxWVlRWSBiGzz/s5yrj4RGeAts01EBmRVJkjx/UNEEr3/v5kiUjqb1172vRDE+F4Rkf2Z/ofdsnntZT/rQYxvSqbYdonImmxeG/T9F3CqGnE/OMNA7wBqAwWBtUDjS8r8DhjpfdwbmBLC+CoDrbyPS+BMSn9pfJ2AWS7uw11A7GXWdwPmAgK0A5a5+L8+hNNH2bX9B1wHtAI2ZFr2d+B57+Pngb9l8bqywE7v7zLex2VCFF9noID38d+yii8n74UgxvcK8EwO/v+X/awHK75L1r8FvOTW/gv0T6Se8bcBtqvqTlW9AEwGelxSpgcwzvv4M+AmCdF8aqp6UFVXeR+fAjYDVUNRdwD1AD5Rx1KgtIhUdiGOm4AdqurqXJyquhhnTonMMr/HxgF3ZvHSLsACVT2mqseBBUDXUMSnqvNVNd37dCkQ2olsfx1LVvsvJ3LyWffb5eLz5o2ewKRA1+uWSE38VYG9mZ7v47eJ9b9lvG/+FKBcSKLLxNvE1BJYlsXq9iKyVkTmikiTkAYGCswXkZXe+Y4vlZN9HAq9yf4D5+b+A6ioqge9jw8BFbMoEy77cRDON7isXOm9EEyPepuixmTTVBYO++9aIElVt2Wz3s3955NITfwRQUSKA9OBJ1X15CWrV+E0XzQH/g18HuLwOqpqK+BWYLiIXBfi+q/IO6XnHcC0LFa7vf9+RZ3v/GHZRU5E/gCkAxOzKeLWe+F9oA7QAjiI05wSju7n8mf7Yf9ZulSkJv79QPVMz6t5l2VZRpyJ3ksBR0MSnVNnDE7Sn6iqMy5dr6onVfW09/EcIEZEYkMVn6ru9/5OBmbifKXOLCf7ONhuBVapatKlK9zef15JF5u/vL+Tsyjj6n4UkQeB7kAf78HpN3LwXggKVU1S1QxV9QAfZFOv2/uvAHA3MCW7Mm7tP39EauIP64nevW2CHwGbVfXtbMpUunjNQUTa4PwvQnJgEpFiIlLi4mOci4AbLin2JdDf27unHZCSqVkjVLI903Jz/2WS+T02APgiizLzgM4iUsbblNHZuyzoRKQr8Bxwh6qezaZMTt4LwYov8zWju7KpNyef9WC6GUhU1X1ZrXRz//nF7avLvv7g9DrZinPF/w/eZa/ivMkBCuM0EWwHlgO1QxhbR5yv/euANd6fbsAwYJi3zKPARpxeCkuBDiGMr7a33rXeGC7uv8zxCfCed/+uB+JD/P8thpPIS2Va5tr+wzkAHQTScNqZB+NcM/oW2AZ8A5T1lo0HPsz02kHe9+F2YGAI49uO0z5+8T14sZdbFWDO5d4LIYpvvPe9tQ4nmVe+ND7v89981kMRn3f52IvvuUxlQ77/Av1jd+4aY0w+E6lNPcYYY3xkid8YY/IZS/zGGJPPWOI3xph8xhK/McbkM5b4jTEmn7HEb4wx+YwlfmOMyWf+P76uG5i+flUvAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC0PJOkV6aYH"
      },
      "source": [
        "# Compute directly the optimal value function for each state\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qab5m6QtGVHK",
        "outputId": "5bec38f6-5bd5-4bbb-b3c2-a6ea5479affa"
      },
      "source": [
        "for t in range(20):\n",
        "    L_reward = []      \n",
        "    for action in range(env.nA):\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        L_reward.append(reward)  #The value of each action\n",
        "    best_value = max(L_reward)\n",
        "    best_action = np.argmax(np.asarray(L_reward))   # choose the action which gives the maximum value\n",
        "    print(\"The optimal value for each state is {} and is get by taking the action {}\".format(best_value,best_action))"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The optimal value for each state is 2.0 and is get by taking the action 1\n",
            "The optimal value for each state is 2.0 and is get by taking the action 1\n",
            "The optimal value for each state is 2.0 and is get by taking the action 1\n",
            "The optimal value for each state is 2.0 and is get by taking the action 1\n",
            "The optimal value for each state is 2.0 and is get by taking the action 1\n",
            "The optimal value for each state is 2.0 and is get by taking the action 1\n",
            "The optimal value for each state is 2.0 and is get by taking the action 1\n",
            "The optimal value for each state is 2.0 and is get by taking the action 1\n",
            "The optimal value for each state is 2.0 and is get by taking the action 1\n",
            "The optimal value for each state is 2.0 and is get by taking the action 1\n",
            "The optimal value for each state is 2.0 and is get by taking the action 1\n",
            "The optimal value for each state is 2.0 and is get by taking the action 1\n",
            "The optimal value for each state is 2.0 and is get by taking the action 1\n",
            "The optimal value for each state is 2.0 and is get by taking the action 1\n",
            "The optimal value for each state is 2.0 and is get by taking the action 1\n",
            "The optimal value for each state is 2.0 and is get by taking the action 1\n",
            "The optimal value for each state is 2.0 and is get by taking the action 1\n",
            "The optimal value for each state is 2.0 and is get by taking the action 1\n",
            "The optimal value for each state is 2.0 and is get by taking the action 1\n",
            "The optimal value for each state is 2.0 and is get by taking the action 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRz0uMX1LbXz"
      },
      "source": [
        "# Implement Value Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVuBLOrWL1ru"
      },
      "source": [
        "  Evaluate the optimal value function given a full description of the environment dynamics\n",
        "  \n",
        "  \n",
        "\n",
        "```\n",
        " Args:\n",
        "\n",
        "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
        "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
        "            env.nS is a number of states in the environment. \n",
        "            env.nA is a number of actions in the environment.\n",
        "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
        "        discount_factor: Gamma discount factor.\n",
        "  \n",
        "  Returns:\n",
        "        Vector of length env.nS representing the value function.\n",
        "```\n",
        "\n",
        "\n",
        "  \n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nia1zMpUZw2j",
        "outputId": "ee832bdb-8bd7-40bd-d2f6-1332f0fec009"
      },
      "source": [
        "def get_expected_reward(env,s,a):\n",
        "  reward=0\n",
        "  nb_possible_action = len(env.P[s][a])\n",
        "  for i in range (nb_possible_action):\n",
        "    reward += env.P[s][a][i][0]*env.P[s][a][i][2]\n",
        "  return (reward/nb_possible_action)\n",
        "\n",
        "get_expected_reward(env,0,0)"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9VxnFu_WkL1",
        "outputId": "bb3b9f62-c5e2-4707-da80-f3e00177cfff"
      },
      "source": [
        "def optimal_value_function (env, theta=0.01, gamma=0.9):\n",
        "  tps1 = time.clock()\n",
        "  V_old = np.zeros(env.nS)\n",
        "  V_new = np.zeros(env.nS)\n",
        "  converge = False\n",
        "  while not converge: \n",
        "    who_converge = 0\n",
        "    V_old = np.copy(V_new)\n",
        "    for s in range(env.nS):\n",
        "      possible_value=[]\n",
        "      for a in range(env.nA):\n",
        "        value_for_a = get_expected_reward(env,s,a) + gamma*sum([ env.P[s][a][k][0]*V_old[env.P[s][a][k][1]] for k in range(len(P[s][a]))])\n",
        "        possible_value.append(value_for_a)\n",
        "      V_new[s]=max(possible_value)\n",
        "      if (abs(V_new[s]-V_old[s])<theta):\n",
        "        who_converge += 1\n",
        "    if (who_converge == env.nS):\n",
        "      converge = True\n",
        "  tps2 = time.clock()\n",
        "  timeConvergence = tps2 - tps1\n",
        "  return (V_new,timeConvergence)\n",
        "\n",
        "\n",
        "optimal_value_function(env, theta = 0.01, gamma = 0.9)\n",
        "      \n"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([9.91272036, 9.91272036]), 0.0008930000000013649)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGWxWGNA6aYJ"
      },
      "source": [
        "# Implement policy iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9o3S2kaKQKX"
      },
      "source": [
        "First a policy evluation\n",
        "\n",
        "```\n",
        "Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
        "    \n",
        "    Args:\n",
        "        policy: [S, A] shaped matrix representing the policy.\n",
        "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
        "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
        "            env.nS is a number of states in the environment. \n",
        "            env.nA is a number of actions in the environment.\n",
        "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
        "        discount_factor: Gamma discount factor.\n",
        "    \n",
        "    Returns:\n",
        "        Vector of length env.nS representing the value function.\n",
        "        \n",
        "```\n",
        "\n",
        "Then a policy improvement:\n",
        "\n",
        "\n",
        "```\n",
        " Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
        "    until an optimal policy is found.\n",
        "    \n",
        "    Args:\n",
        "        env: The OpenAI envrionment.\n",
        "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
        "            policy, env, discount_factor.\n",
        "        discount_factor: gamma discount factor.\n",
        "        \n",
        "    Returns:\n",
        "        A tuple (policy, V). \n",
        "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
        "        contains a valid probability distribution over actions.\n",
        "        V is the value function for the optimal policy.\n",
        "        \n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvV3BN2YiYGu",
        "outputId": "7a9eb5cf-f862-4c15-accd-730c74c2b66e"
      },
      "source": [
        "def value_policy (policy, env, theta=0.01, gamma=0.9):\n",
        "  V_old = np.zeros(env.nS)\n",
        "  V_new = np.zeros(env.nS)\n",
        "  converge = False\n",
        "  while not converge: \n",
        "    who_converge = 0\n",
        "    V_old = np.copy(V_new)\n",
        "    for s in range(env.nS):\n",
        "      a= np.argmax(policy[s,:])\n",
        "      value_for_a = get_expected_reward(env,s,a) + gamma*sum([ env.P[s][a][k][0]*V_old[env.P[s][a][k][1]] for k in range(len(P[s][a]))])\n",
        "      V_new[s]=value_for_a\n",
        "      if (abs(V_new[s]-V_old[s])<theta):\n",
        "        who_converge += 1\n",
        "    if (who_converge == env.nS):\n",
        "      converge = True\n",
        "  return V_new\n",
        "\n",
        "policy=np.array([[0,1,0],[0,1,0]])\n",
        "\n",
        "value_policy(policy,env, theta = 0.01, gamma = 0.9)\n",
        "      \n"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9.91272036, 9.91272036])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1aWCTj_jtdX",
        "outputId": "5b0d6e44-26ee-4224-c1c7-a7d1179d75de"
      },
      "source": [
        "def policy_improvement(env, value_policy=value_policy, gamma=0.9):\n",
        "  tps1 = time.clock()\n",
        "  list_policy=[]\n",
        "  list_value=[]\n",
        "  list_sum_value=[]\n",
        "  for k in range (env.nA**env.nS):\n",
        "    policy = np.zeros([env.nS,env.nA])\n",
        "    for p in range (env.nS):\n",
        "      policy[p][0]=0\n",
        "      #f=max(2*k+p,k)\n",
        "      if (p%2==0):\n",
        "        j=k//env.nA\n",
        "      else:\n",
        "        j=k%env.nA\n",
        "      policy[p][j]=1\n",
        "\n",
        "    values=value_policy(policy,env)\n",
        "\n",
        "    list_policy.append(policy)\n",
        "    list_value.append(values)\n",
        "    list_sum_value.append(np.sum(values))\n",
        "\n",
        "  idx = np.argmax(np.array(list_sum_value))\n",
        "  tps2 = time.clock()\n",
        "  timeConvergence = tps2 - tps1\n",
        "  \n",
        "  return (list_policy[idx], list_value[idx],timeConvergence)\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "policy_improvement(env)[0]\n"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [0., 1., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brOjUJvE6aYV"
      },
      "source": [
        "# Using the 3 algorithms do the following experiments\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E911dO152K3P"
      },
      "source": [
        "def optimal_strategy(env):\n",
        "  tps1 = time.clock()\n",
        "  totalReward = 0\n",
        "  Reward = []\n",
        "  CumuReward =[]\n",
        "  observation = env.reset()\n",
        "  policy = policy_improvement(env)[0]\n",
        "  #print('The initial obsevation is :',  observation, 'so we are in the state', states[observation])\n",
        "  for i in range(20):\n",
        "    action= np.argmax(policy[observation,:])\n",
        "    #action = env.action_space.sample() # your agent here (this takes random actions)\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    totalReward += reward\n",
        "    Reward.append(reward)\n",
        "    CumuReward.append(totalReward)\n",
        "    #print('obsevation after step number ', i, 'is :',  observation, 'so we are in the state', states[observation],'The action taken is : ',  actions[action], ' and the reward  is: ', reward)\n",
        "  tps2 = time.clock()\n",
        "  timeConvergence = tps2 - tps1\n",
        "  return (totalReward, timeConvergence, Reward, CumuReward)\n"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVKvrm0t6aYV"
      },
      "source": [
        "env = gen_ambient(alpha=1, beta=1, r_search=0.5, r_wait=2)\n",
        "exp1 = gen_ambient(alpha=0.9, beta=0.9, r_search=3, r_wait=2)\n",
        "exp2 = gen_ambient(alpha=0.8, beta=0.5, r_search=3, r_wait=2)\n",
        "exp3 = gen_ambient(alpha=0.5, beta=0.5, r_search=3, r_wait=2)\n",
        "exp4 = gen_ambient(alpha=0.9, beta=0.6, r_search=1, r_wait=0.9)\n",
        "exp5 = gen_ambient(alpha=0.9, beta=0.6, r_search=1, r_wait=0.5)\n"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAT_YZq56aYY"
      },
      "source": [
        "# Compare the different strategies with the random one\n",
        "# Compare the different strategies in terms of speed of convergence for the different scenarios\n",
        "# What would you do if alpha and beta are unknown (and you dont know RL)? Try to implement something if you have time.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfpeI2gx2peI",
        "outputId": "20f3e1e9-140c-4ebf-cfb4-e0162808582b"
      },
      "source": [
        "totalRewardRandom,timeConvergenceRandom = random_strategy(env)[0:2]\n",
        "totalRewardOptimal,timeConvergenceOptimal = optimal_strategy(env)[0:2]\n",
        "\n",
        "print(totalRewardRandom,timeConvergenceRandom)\n",
        "print(totalRewardOptimal,timeConvergenceOptimal)"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7.5, 0.000408000000000186)\n",
            "(10.0, 0.005973000000000894)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-xoMvolGC1k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbcfca19-4dde-4e01-a81d-0dcf9bb6952e"
      },
      "source": [
        "totalRewardRandom,timeConvergenceRandom = random_strategy(exp1)[0:2]\n",
        "totalRewardOptimal,timeConvergenceOptimal = optimal_strategy(exp1)[0:2]\n",
        "\n",
        "print(totalRewardRandom,timeConvergenceRandom)\n",
        "print(totalRewardOptimal,timeConvergenceOptimal)"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8.5, 0.0004960000000018283)\n",
            "(10.0, 0.006088999999999345)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_JKpgPxYd8o",
        "outputId": "e1da03db-0793-46e3-8a63-7ef541ddeaa8"
      },
      "source": [
        "totalRewardRandom,timeConvergenceRandom = random_strategy(exp2)[0:2]\n",
        "totalRewardOptimal,timeConvergenceOptimal = optimal_strategy(exp2)[0:2]\n",
        "\n",
        "print(totalRewardRandom,timeConvergenceRandom)\n",
        "print(totalRewardOptimal,timeConvergenceOptimal)"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8.5, 0.002192999999998335)\n",
            "(10.0, 0.006596000000001823)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9MUvA74Z2ce",
        "outputId": "84c0c996-7180-496e-8e5f-9ffe887c1294"
      },
      "source": [
        "totalRewardRandom,timeConvergenceRandom = random_strategy(exp3)[0:2]\n",
        "totalRewardOptimal,timeConvergenceOptimal = optimal_strategy(exp3)[0:2]\n",
        "\n",
        "print(totalRewardRandom,timeConvergenceRandom)\n",
        "print(totalRewardOptimal,timeConvergenceOptimal)"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10.0, 0.0010290000000026112)\n",
            "(10.0, 0.0060789999999997235)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAFhB1EkZ2nW",
        "outputId": "e7c98318-2bfc-49c8-cec1-a1dfef7443af"
      },
      "source": [
        "totalRewardRandom,timeConvergenceRandom = random_strategy(exp4)[0:2]\n",
        "totalRewardOptimal,timeConvergenceOptimal = optimal_strategy(exp4)[0:2]\n",
        "\n",
        "print(totalRewardRandom,timeConvergenceRandom)\n",
        "print(totalRewardOptimal,timeConvergenceOptimal)"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7.5, 0.0011970000000012249)\n",
            "(10.0, 0.005272999999998973)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84__mGieZ2ux",
        "outputId": "a6e58836-ffe4-417b-a3bf-4344de939306"
      },
      "source": [
        "totalRewardRandom,timeConvergenceRandom = random_strategy(exp5)[0:2]\n",
        "totalRewardOptimal,timeConvergenceOptimal = optimal_strategy(exp5)[0:2]\n",
        "\n",
        "print(totalRewardRandom,timeConvergenceRandom)\n",
        "print(totalRewardOptimal,timeConvergenceOptimal)"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10.5, 0.0017149999999972465)\n",
            "(10.0, 0.00629199999999841)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9hn6_767OmX"
      },
      "source": [
        "**Difference of speed of convergence**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDqmxHER7Sy7",
        "outputId": "62aed454-700d-470b-9ad7-7d83cf613248"
      },
      "source": [
        "#env\n",
        "\n",
        "timeConvergence1 = optimal_value_function(env)[1]\n",
        "timeConvergence2 = policy_improvement(env)[2]\n",
        "\n",
        "print('optimal value function:', timeConvergence1)\n",
        "print('policy improvement:', timeConvergence2)"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('optimal value function:', 0.0019700000000000273)\n",
            "('policy improvement:', 0.004179999999998074)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8TjzOiy78V2",
        "outputId": "5f747741-96c2-4277-cf73-94be96eda16c"
      },
      "source": [
        "#exp1\n",
        "\n",
        "timeConvergence1 = optimal_value_function(exp1)[1]\n",
        "timeConvergence2 = policy_improvement(exp1)[2]\n",
        "\n",
        "print('optimal value function:', timeConvergence1)\n",
        "print('policy improvement:', timeConvergence2)"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('optimal value function:', 0.0028849999999991383)\n",
            "('policy improvement:', 0.004308000000001755)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJ9J_LdM78sm",
        "outputId": "39b1530c-0f97-42e7-fb26-62f3e335f15a"
      },
      "source": [
        "#exp2\n",
        "\n",
        "timeConvergence1 = optimal_value_function(exp2)[1]\n",
        "timeConvergence2 = policy_improvement(exp2)[2]\n",
        "\n",
        "print('optimal value function:', timeConvergence1)\n",
        "print('policy improvement:', timeConvergence2)"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('optimal value function:', 0.004248000000000474)\n",
            "('policy improvement:', 0.006061000000002537)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzKCOUFD78vo",
        "outputId": "e066d1e0-0753-43d5-cf8d-3f0178b6d8f8"
      },
      "source": [
        "#exp3\n",
        "\n",
        "timeConvergence1 = optimal_value_function(exp3)[1]\n",
        "timeConvergence2 = policy_improvement(exp3)[2]\n",
        "\n",
        "print('optimal value function:', timeConvergence1)\n",
        "print('policy improvement:', timeConvergence2)"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('optimal value function:', 0.0013970000000007587)\n",
            "('policy improvement:', 0.004709000000001851)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgREnRNv78yd",
        "outputId": "2c24503e-9c21-4ff7-ff02-be641e7c6d57"
      },
      "source": [
        "#exp4\n",
        "\n",
        "timeConvergence1 = optimal_value_function(exp4)[1]\n",
        "timeConvergence2 = policy_improvement(exp4)[2]\n",
        "\n",
        "print('optimal value function:', timeConvergence1)\n",
        "print('policy improvement:', timeConvergence2)"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('optimal value function:', 0.0022359999999999047)\n",
            "('policy improvement:', 0.00513300000000072)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gB-3XBlf7807",
        "outputId": "96ff0662-85d2-4d91-9d31-f538b9165d2a"
      },
      "source": [
        "#exp5\n",
        "\n",
        "timeConvergence1 = optimal_value_function(exp5)[1]\n",
        "timeConvergence2 = policy_improvement(exp5)[2]\n",
        "\n",
        "print('optimal value function:', timeConvergence1)\n",
        "print('policy improvement:', timeConvergence2)"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('optimal value function:', 0.0030420000000006553)\n",
            "('policy improvement:', 0.006017000000003492)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2RiQWO58Wl7"
      },
      "source": [
        "The optimal value method seems to be twice faster than the policy improvement"
      ]
    }
  ]
}